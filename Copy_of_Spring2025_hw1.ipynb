{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "fvoUaj5nNAh2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ananya1306/applied-machine-learning-algorithms-3806104/blob/main/Copy_of_Spring2025_hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKc8K9_57UEz"
      },
      "source": [
        "# CS229-EE242 - Spring 2025 - Homework 1\n",
        "\n",
        "# Due: Friday, April 25, 2025 @ 11:59pm\n",
        "\n",
        "### Maximum points: 50 pts\n",
        "\n",
        "\n",
        "## Submit your solution to elearn:\n",
        "1. Submit a single PDF to **HW1**\n",
        "2. Submit your jupyter notebook to **HW1-code**\n",
        "\n",
        "**Both code and pdf need to show code outputs. See the additional submission instructions at the end of this notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL5tIX4c9z6s"
      },
      "source": [
        "\n",
        "## Enter your information below:\n",
        "\n",
        "### Your Name (submitter):\n",
        "\n",
        "### Your student ID (submitter):\n",
        "    \n",
        "    \n",
        "<b>By submitting this notebook, I assert that the work below is my own work, completed for this course.  Except where explicitly cited, none of the portions of this notebook are duplicated from anyone else's work or my own previous work.</b>\n",
        "\n",
        "\n",
        "## Academic Integrity\n",
        "Each assignment should be done  individually. You may discuss general approaches with other students in the class, and ask questions to the TAs, but  you must only submit work that is yours . If you receive help by any external sources (other than the TA and the instructor), you must properly credit those sources, and if the help is significant, the appropriate grade reduction will be applied. If you fail to do so, the instructor and the TAs are obligated to take the appropriate actions outlined at http://conduct.ucr.edu/policies/academicintegrity.html . Please read carefully the UCR academic integrity policies included in the link.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch Materials:\n",
        "\n",
        "If you're not familiar with PyTorch, please review the introductory tutorials before proceeding with this notebook:\n",
        "\n",
        "- [PyTorch Beginner Tutorials](https://pytorch.org/tutorials/beginner/basics/intro.html)\n",
        "- [PyTorch Neural Networks Tutorial](https://pytorch.org/tutorials/beginner/nn_tutorial.html)\n",
        "\n",
        "You can also refer to this notebook for an introduction:  \n",
        "[PyTorch Tutorial - CS231N Spring 2024](https://colab.research.google.com/drive/1FERNv6t8xpX9Nly_JdnePWEPllI7F3Fx?usp=sharing)"
      ],
      "metadata": {
        "id": "pZegGMjR6Q-s"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsywQWEI8pzj"
      },
      "source": [
        "# Vision Transformer (ViT) Homework\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this homework, we will build and train a Vision Transformer (ViT)[1] from scratch. You will implement key components, including **patch embedding**, **normalizations**, and **self-attention layers**, and **train** the model on an image classification task. Additionally, you will train the model on randomly assigned labels to analyze its generalization behavior.\n",
        "\n",
        "<!-- <p align=\"center\">\n",
        "  \n",
        "</p> -->\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2021-01-26_at_9.43.31_PM_uI4jjMq.png\" />\n",
        "  <img src=\"https://theaisummer.com/static/aa65d942973255da238052d8cdfa4fcd/7d4ec/the-transformer-block-vit.png\" alt=\"Vision Transformer Architecture\" />\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  Figure 1. Vision Transformer Architecture\n",
        "</p>\n",
        "\n",
        "\n",
        "### Expected Outcomes\n",
        "\n",
        "- Understanding of Vision Transformer architecture.\n",
        "- Insights into model memorization and generalization behavior.\n",
        "\n",
        "### References\n",
        "\n",
        "[1] Dosovitskiy, Alexey, et al. \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\" *International Conference on Learning Representations (ICLR)*, 2020.  \n",
        "\n",
        "\n",
        "## Read *all* cells carefully and answer all parts\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n"
      ],
      "metadata": {
        "id": "C4Lctod46zDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1. Implement and train the MLP Block [8 pts]\n"
      ],
      "metadata": {
        "id": "s_ZNX4mfLaDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1.1 Implement the MLP Block [3 pts]\n",
        "\n",
        "The MLP accepts input as a vector and contains two layers with a GELU non-linearity.\n",
        "\n",
        "**TODO**:\n",
        "Complete the implementation of the MLPBlock class. You should define the layers in the constructor (__init__) and implement the forward method (forward). The module will have two linear layers and activation layer in between.\n",
        "\n",
        "**Hints:**\n",
        "- **Linear Layers**: Use `nn.Linear` to map from `in_dim` → `hidden_dim`, then from `hidden_dim` → `out_dim`.\n",
        "- **Activation**: Insert an activation function (e.g., `nn.ReLU()` or `nn.GELU()`) between the linear layers.\n",
        "- **Forward**: Apply the layers sequentially: first the **linear layer**, then **activation**, then the **second linear layer**.\n"
      ],
      "metadata": {
        "id": "I04wxNE95TsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPBlock(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
        "        \"\"\"\n",
        "        Initialize the MLP block with two linear layers and an activation function.\n",
        "\n",
        "        Args:\n",
        "            in_dim (int): Input feature dimension.\n",
        "            hidden_dim (int): Hidden layer dimension.\n",
        "            out_dim (int): Output feature dimension.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO: Define the layers here\n",
        "        # self.layers =\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the MLP block.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, in_dim).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, out_dim).\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass\n",
        "        return x"
      ],
      "metadata": {
        "id": "nCMRSFXV3JBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1.2 Train the MLP Block [5 pts]\n",
        "\n",
        "In this section, we will train a simple **2-layer MLP (Multilayer Perceptron)** for image classification using the CIFAR-10 dataset. The model will incorporate a GeLU (Gaussian Error Linear Unit) activation function, which is commonly used in transformer architectures.\n",
        "\n",
        "**Steps:**\n",
        "1. **Dataset**: We'll use the CIFAR-10 dataset, which contains 60,000 32x32 color images in 10 classes.\n",
        "2. **Model**: Model: The model will be a basic 2-layer MLP, with a GeLU activation function between the layers\n",
        "3. **Training**: Train the model for some epochs using Cross-Entropy loss and Adam optimizer.\n",
        "4. **Evaluation**: Evaluate the model's performance on the test set."
      ],
      "metadata": {
        "id": "oGDNf5eFLiaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset and dataloading\n",
        "\n",
        "In the code below, we will define the dataset and dataloader, which will be used in our training loop.\n",
        "\n",
        "Note that we are using `batch_size = 16`, you can use the same batch size or play with this number.\n",
        "\n"
      ],
      "metadata": {
        "id": "rfRE_b5PML_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=16)"
      ],
      "metadata": {
        "id": "kBvnsRPYMMKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define the model [1 pts]\n",
        "\n",
        "In this section, we will define a classification model using a simple 2-layer MLP. The first layer transforms the input, followed by a GeLU activation, and the second layer produces the final output for classification.\n",
        "\n",
        "**Hints:**\n",
        "- The input dimension is the total number of pixels in the image, including all color channels. For CIFAR-10, `in_dim = 32 * 32 * 3 = 3072`.\n",
        "- Use `.view(x.size(0), -1)` to flatten the image into a 1D vector for the MLP.\n",
        "- The size of the hidden layer (`hidden_dim`) is a hyperparameter. You can experiment with different sizes to find what works best.\n",
        "- The size of `out_dim` will be 10, because we want to estimate a probability vector with 10 classes.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fvoUaj5nNAh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model ="
      ],
      "metadata": {
        "id": "wC_1IfMYNH21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implement training loop [2 pts]\n",
        "\n",
        "In this section, you will implement the training loop for the MLP model. Below is a basic structure for the training process. We have implemented some parts and you will fill in the rest.\n",
        "\n",
        "- **Training Loop:**: Implement the loop to train the model for some epochs, calculate the loss, and update the model parameters.\n",
        "\n",
        "- **Evaluation**: Compute and store `test_accuracy` and `train_accuracy` per each epoch.\n",
        "\n",
        "\n",
        "**You should get at least 50% accuracy on the test data.**\n"
      ],
      "metadata": {
        "id": "4w88mK40MrsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer and loss function\n",
        "learning_rate = 2e-4\n",
        "num_epochs = 20\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "\n",
        "test_accuracy = []\n",
        "train_accuracy = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        # Move data to device if cuda is available\n",
        "        if torch.cuda.is_available():\n",
        "          inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # TODO:\n",
        "        # Fill the following sections\n",
        "        # Forward pass\n",
        "        # Compute the loss\n",
        "        # Backward propagation on the loss\n",
        "        # Update model parameters using optimizer\n",
        "\n",
        "        outputs =\n",
        "        loss =\n",
        "\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # TODO\n",
        "    # Compute accuracy on the training set\n",
        "\n",
        "\n",
        "    # Print the average loss for the epoch\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            # Move data to device if cuda is available\n",
        "            if torch.cuda.is_available():\n",
        "              inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # TODO:\n",
        "            # Fill the following sections\n",
        "\n",
        "            # Forward pass\n",
        "            outputs =\n",
        "\n",
        "            # Get predictions\n",
        "            predicted =\n",
        "\n",
        "\n",
        "    # TODO:\n",
        "    # Compute accuracy on the test set\n",
        "\n",
        "    accuracy =\n",
        "    test_accuracy.append(accuracy)\n",
        "    print(f'Accuracy on the test set: {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "liuAUlH1MrA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.3 Plot training and test accuracy [2 pts]\n",
        "\n",
        "In your training loop above, make sure to calculate training accuracy per epoch and append it to the `train_accuracy` and `test_accuracy` variables.\n",
        "\n",
        "We have written the plotting code in the cell below."
      ],
      "metadata": {
        "id": "hPWBT7YmQQEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "axs[0].plot(train_accuracy, label='Train Accuracy', color='blue')\n",
        "axs[0].set_title('Train Accuracy vs. Epoch')\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[0].set_ylabel('Accuracy (%)')\n",
        "axs[0].grid(True)\n",
        "axs[0].legend()\n",
        "\n",
        "axs[1].plot(test_accuracy, label='Test Accuracy', color='green')\n",
        "axs[1].set_title('Test Accuracy vs. Epoch')\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[1].set_ylabel('Accuracy (%)')\n",
        "axs[1].grid(True)\n",
        "axs[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vb6FftcIPznT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2. Implement Patch Embedding Layer [5 pts]\n",
        "\n",
        "Now, we will implement a patch embedding layer that divides each input image into fixed-size patches. Then we will embed each patch independently into a vector (aka token). All the tokens corresponding to all the patches will then be used inputs to the transformer.\n",
        "\n",
        "**Hints:**\n",
        "- **Patch Size:** The image is typically divided into square patches of size `(patch_size, patch_size)`. For example, if the image is 224x224 and the patch size is 16, there will be 224/16 = 14 patches along each dimension, resulting in `N=14 x 14 = 196` patches. CIFAR images are small (32 x 32 pixels), so you may want to use a smaller patch size.\n",
        "\n",
        "- **Conv2D**: Suppose we want to divide an `3 x H x W` image into `N` patches (each of size `3 x p x p`) and then transform each patch into a `d`-dimensional vector (token). In the code below, we refer to `d` as `embed_dim` or `hidden_dim`. We can do all of this using a single convolutional layer (nn.Conv2d) with `d` channels. The kernel size should be equal to the patch size, and the stride should be the same to avoid overlap. The resulting `(d x H/p x W/p)` tensor will have `d` channels, each with `(H/p x W/p)` spatial features.\n",
        "\n",
        "\n",
        "\n",
        "- **Flatten**: After embedding the patches, we can flatten them to create a sequence of patch embeddings. This will be done across the last two dimensions (height, width) after convolution. \\\\\n",
        "Suppose we are using a batch of shape `(batch_size, 3, H, W)`, after Conv2D and Flatten, we should get a tensor of shape `(batch_size, d, N)`.\n",
        "\n",
        "- **Transpose**: After flattening, transpose the output to `(batch_size, num_patches, hidden_dim)` or `(batch_size, N, d)` for the final format."
      ],
      "metadata": {
        "id": "SBaXmurx5q8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, patch_size, in_channels, embed_dim):\n",
        "        \"\"\"\n",
        "        Initialize the Patch Embedding layer.\n",
        "\n",
        "        Args:\n",
        "            patch_size (int): Size of each patch (patch_size x patch_size).\n",
        "            in_channels (int): Number of input channels (This will be 3 for RGB images).\n",
        "            embed_dim (int): Dimension of the embedding for each patch.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO: Define the layers here\n",
        "        # self.layers =\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the PatchEmbedding layer.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, num_patches, embed_dim).\n",
        "\n",
        "            num_patches = (height * width) / (patch_size * patch_size)\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass\n",
        "        return x"
      ],
      "metadata": {
        "id": "H3ehsZl56IAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3. Implement Multi-Head Self-Attention (MHSA) [10 pts]\n",
        "\n",
        "In this task, we will implement a Multi-Head Self-Attention (MHSA) layer. As the name suggests, MHSA has self attention with multiple heads.\n",
        "\n",
        "**Self-Attention** module seeks to combine the input sequence of tokens (based on some similarity metric) to generate a set of output tokens. The key equations for self-attention can be written as follows.\n",
        "\n",
        "For a given $N\\times d$ input token matrix $Z$, with $N$ tokens each of dimension $d$, we generate Query, Key, and Value matrices that can be written as\n",
        "  $$\n",
        "  Q = Z W_Q, \\quad K = Z W_K, \\quad V = Z W_V,\n",
        "  $$\n",
        "where  $W_Q, W_K, W_V$ are weight matrices that we learn for the query, key, and value projections. You can implement each of them as a linear layer.\n",
        "  \n",
        "For simplicity, we can assume that each of the projection matrices have same size (e.g., $d\\times d$ matrices that map each $d$-dim token to $d$-dim space; shortly we will see that in the case of multiple heads we prefer to use a reduced dimension for each head).\n",
        "\n",
        "**Scaled Dot-Product Attention** is computed as\n",
        "\n",
        "  $$\n",
        "  \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d}}\\right) V\n",
        "  $$\n",
        "\n",
        "  Where:\n",
        "  - $d$ is the embedding dimension of tokens in the Q, K, and V matrices. In the code, we also refer to it as `hidden_dim`.\n",
        "  - The softmax function is applied over the rows of $\\frac{Q K^T}{\\sqrt{d}}$.\n",
        "\n",
        "**Multi-Head Selft Attention (MHSA)**: As the saying goes multiple heads are better than one. In practice, we use multiple heads to perform a separate attention operation on each heads with a different set of learned projections.\n",
        "\n",
        "Let us assume that we want to use $h$ heads, all of which receive the same input tokens $Z$ and provide a set of outputs that we denote as $(\\text{head}_1,\\ldots, \\text{head}_h)$. We compute each output head as\n",
        "\n",
        "$$\\text{head}_i = \\text{Attention}(Q_i, K_i, V_i),$$\n",
        "\n",
        "where $Q_i=ZW^i_Q, K_i = Z W^i_K, V_i = ZW^i_V$ are the (Q, K, V) for head i with learnable weight matrices: $W_Q^i, W_K^i, W_V^i$.\n",
        "\n",
        "The choice of embedding dimension for each head is in our control, but for simplicity we use $d_k = d/h$ as the embedding dimension for each head. For this reason, we prefer to use $h$ that divides $d$ into an integer.\n",
        "\n",
        "To compute the final MHSA output, we concatednate the output of all the heads and apply an output linear transform to get a $N\\times d$ output token matrix as\n",
        "  \n",
        "  $$\n",
        "  \\text{MHSA}(Z) = \\text{concat}(\\text{head}_1, \\dots, \\text{head}_h) W_O,\n",
        "  $$\n",
        "where $W_O$ denotes the $d\\times d$ output transform matrix."
      ],
      "metadata": {
        "id": "fmojWexz5vHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MHSA(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads):\n",
        "        \"\"\"\n",
        "        Initialize the Multi-Head Self-Attention (MHSA) layer.\n",
        "\n",
        "        Args:\n",
        "            hidden_dim (int): Embedding dimension for the input patches ($d$).\n",
        "            num_heads (int): Number of attention heads.\n",
        "        \"\"\"\n",
        "        super(MHSA, self).__init__()\n",
        "        # TODO: Define the layers here\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the Multi-Head Self-Attention (MHSA) layer.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, num_patches, hidden_dim).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after applying multi-head attention.\n",
        "        \"\"\"\n",
        "        # TODO:\n",
        "        # 1. Apply linear projections to input x for to obtain Q, K, and V.\n",
        "        # 2. Compute attention scores for each head.\n",
        "        # 3. Apply attention to V: output = attention @ V\n",
        "        # 4. Concatenate outputs from all heads and apply final linear projection\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "0R-i5hYDmYmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4. Build the Vision Transformer Model [10 pts]\n"
      ],
      "metadata": {
        "id": "d-51eAhOuxsA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 4.1 Implement the Encoder Module [5 pts]\n",
        "\n",
        "The EncoderLayer is a core building block of the Vision Transformer (ViT). Each encoder layer consists of two key components: Multi-Head Self-Attention (MHSA) and a Multi-Layer Perceptron (MLP). Each of these components is followed by residual connections and [layer normalization](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#layernorm) (take a look at Figure 1 for visual reference).\n",
        "\n",
        "The `attn_drop_out` argument controls the dropout rate applied to the attention weights. If you set `attn_drop_out=0`, it should disable attention dropout. Use `nn.Dropout` to implement it.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZNOwQWiv6DXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads, mlp_hidden_dim, attn_drop_out = 0):\n",
        "        \"\"\"\n",
        "          An encoder layer in the Vision Transformer (ViT).\n",
        "\n",
        "          Args:\n",
        "              hidden_dim (int): The dimension of the embedding space ($d$).\n",
        "              num_heads (int): The number of attention heads in the Multi-Head Self-Attention (MHSA) layer.\n",
        "              mlp_hidden_dim (int): Hidden dimension of the MLP block.\n",
        "              attn_drop_out (float): Dropout rate applied to the attention weights. Set to 0 to disable dropout and allow full attention flow during training.\n",
        "        \"\"\"\n",
        "\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        # TODO define Multi-Head Self-Attention (MHSA)\n",
        "        self.mhsa =\n",
        "\n",
        "        # TODO define Layer normalization\n",
        "        self.norm1 =\n",
        "\n",
        "        # TODO define MLP\n",
        "        self.mlp =\n",
        "\n",
        "        # TODO define a second Layer normalization\n",
        "        self.norm2 =\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the Encoder layer.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, num_patches, embed_dim).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Encoder output\n",
        "        \"\"\"\n",
        "        # TODO:\n",
        "        # 1. Apply MHSA to x\n",
        "        # 2. Perform layer normalization with residual connection\n",
        "        # 3. Perform projection using MLP with residual connection\n",
        "        # 4. Compute output by performing second layer normalization\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "FDH0VvG06IvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Build the Vision Transformer Model [5 pts]\n",
        "\n",
        "We have almost everything ready to build a vision transformer. We need to take care of two things: 1) positional encoding; 2) `cls` embedding token\n",
        "\n",
        "**Positional encoding.** Note that the attention mechanism is permuation invariant (i.e., we can shuffle the patches in a random order and get the same output from the attention layers). To encode positional information in the patches, we add position embedding vectors to all the input embedded patches. The position embedding vectors have the same dimension ($d$) as the embedded patches. A common choice for position embedding is the sinusoidal embedding, but below we will use learnable embedding.\n",
        "\n",
        "**CLS token.** As we discussed in the class that we usually prepend an extra learnable `cls` token to the input embedding (as shown in Figure 1). We then use the output token corresponding to the `cls` token at the last layer to perform the classification. The total number of tokens in the sequence will therefore be `num_patches+1`.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U3M7b-29unH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, num_heads, num_layers, hidden_dim, num_classes, patch_size, in_channels, img_size, mlp_hidden_dim, attn_drop_out):\n",
        "        \"\"\"\n",
        "        Vision Transformer (ViT) model.\n",
        "\n",
        "        Args:\n",
        "            num_heads (int): The number of attention heads in each Multi-Head Self-Attention (MHSA) layer.\n",
        "            num_layers (int): The number of transformer encoder layers to be stacked in the model.\n",
        "            hidden_dim (int): The dimension of the embedding space ($d$).\n",
        "            num_classes (int): The number of output classes for the classification task.\n",
        "            patch_size (int): The size of the patches that the input image will be divided into (e.g., 16x16).\n",
        "            in_channels (int): The number of channels of the input image (e.g, 3 for RGB images).\n",
        "            img_size (int): The size of the input image (e.g., 224 for 224x224 images).\n",
        "            mlp_hidden_dim (int): The hidden dimension of the MLP block.\n",
        "            attn_drop_out (float): Dropout rate applied to the attention weights. Set to 0 to disable dropout and allow full attention flow during training.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        super(VisionTransformer, self).__init__()\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_dim)) # A class Token (Learnable Parameter)\n",
        "\n",
        "        # Learnable Position embedding\n",
        "        seq_length = (img_size // patch_size) ** 2 + 1\n",
        "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_dim).normal_(std=0.02))\n",
        "\n",
        "        # Complete the rest of the layers\n",
        "\n",
        "        # TODO: Create a Patch Embedding Layer\n",
        "        self.patch_embed =\n",
        "\n",
        "        # TODO: Create `num_layers` Encoder Layers\n",
        "        self.encoder_layers =\n",
        "\n",
        "        # TODO: Create a Final Linear Layer for Classification\n",
        "        self.fc =\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the ViT model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, num_patches, embed_dim).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: ViT output\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        # 1. Apply patch embedding and concatenate class token\n",
        "        # 2. Add position encoding and perform forward pass through each encoder layer\n",
        "        # 3. Extract the class token's output for classification and pass it through the final classification layer\n",
        "        # 4. Return output\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "LRDXfiBCu8Du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5. Training and Evaluation [17 pts]"
      ],
      "metadata": {
        "id": "SGz4kSrX6LEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Image classification [10 pts]\n",
        "\n",
        "In this section, we will implement a simple image classification model using the CIFAR-10 dataset. The model will be a **2-layer Vision Transformer (ViT)**, where the images are divided into patches, and a Transformer encoder is used for classification.\n",
        "\n",
        "**Steps:**\n",
        "1. **Dataset**: We'll use the CIFAR-10 dataset, which contains 60,000 32x32 color images in 10 classes.\n",
        "2. **Model**: A basic 2-layer Vision Transformer (ViT) with patch embedding, Transformer layers, and a classification head.\n",
        "3. **Training**: Train the model for 5 epochs using Cross-Entropy loss and Adam optimizer.\n",
        "4. **Evaluation**: Evaluate the model's performance on the test set.\n"
      ],
      "metadata": {
        "id": "4gX6Ig8MzoJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data loading and training setup"
      ],
      "metadata": {
        "id": "iqa6BoUSPgat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=16)"
      ],
      "metadata": {
        "id": "_lcJPH-QQKHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.1.1 Define the model [3 pts]\n",
        "\n",
        "In this section, you will create an object of the VisionTransformer class that you implemented in **Question 4.**\n",
        "\n",
        "Use the following parameters to instantiate the model:\n",
        "- `num_heads`: Number of attention heads in each transformer layer ($h$).\n",
        "- `num_layers`: Number of transformer layers.\n",
        "- `hidden_dim`: The dimensionality of the embedding space ($d$).\n",
        "- `num_classes`: The number of output classes (CIFAR-10 has 10 classes).\n",
        "- `patch_size`: The size of the patches to divide the image.\n",
        "- `in_channels`: The number of input channels (3 for RGB images).\n",
        "- `img_size`: The size of the input images (32x32 for CIFAR-10).\n",
        "- `mlp_hidden_dim`: The hidden dimension of the MLP\n",
        "- `attn_drop_out`: Dropout rate applied to the attention weights.\n"
      ],
      "metadata": {
        "id": "Ibxdq5T7PmKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example values for these parameters can be following but we strongly encourage you try your own values.\n",
        "num_heads = 8\n",
        "num_layers = 4\n",
        "hidden_dim = 256\n",
        "num_classes = 10\n",
        "patch_size = 4\n",
        "in_channels = 3\n",
        "img_size = 32\n",
        "mlp_hidden_dim = 256*4\n",
        "attn_drop_out = 0.1\n"
      ],
      "metadata": {
        "id": "7M5-0hDyP6MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "num_heads =\n",
        "num_layers =\n",
        "hidden_dim =\n",
        "num_classes =\n",
        "patch_size =\n",
        "in_channels =\n",
        "img_size =\n",
        "mlp_hidden_dim =\n",
        "attn_drop_out =\n",
        "\n",
        "# Instantiate the model\n",
        "model ="
      ],
      "metadata": {
        "id": "W3ddnfr2bfzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.1.2 Training loop [5 pts]\n",
        "\n",
        "In this section, you will implement the training loop for the Vision Transformer model. Below is a basic structure for the training process. We have implemented some parts and you will fill in the rest.\n",
        "\n",
        "- **Training Loop:**: Implement the loop to train the model for some epochs, calculate the loss, and update the model parameters.\n",
        "\n",
        "- **Evaluation**: Compute and store `test_accuracy` and `train_accuracy` per each epoch.\n",
        "\n",
        "\n",
        "**You should get at least 75% accuracy on the test data.**"
      ],
      "metadata": {
        "id": "wNRscZQsPzLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer and loss function\n",
        "learning_rate = 2e-4\n",
        "num_epochs = 20\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "\n",
        "test_accuracy = []\n",
        "train_accuracy = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        # Move data to device if cuda is available\n",
        "        if torch.cuda.is_available():\n",
        "          inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # TODO:\n",
        "        # Fill the following sections\n",
        "        # Forward pass\n",
        "        # Compute the loss\n",
        "        # Backward propagation on the loss\n",
        "        # Update model parameters using optimizer\n",
        "\n",
        "        outputs =\n",
        "        loss =\n",
        "\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # TODO\n",
        "    # Compute accuracy on the training set\n",
        "\n",
        "\n",
        "    # Print the average loss for the epoch\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            # Move data to device if cuda is available\n",
        "            if torch.cuda.is_available():\n",
        "              inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # TODO:\n",
        "            # Fill the following sections\n",
        "\n",
        "            # Forward pass\n",
        "            outputs =\n",
        "\n",
        "            # Get predictions\n",
        "            predicted =\n",
        "\n",
        "\n",
        "    # TODO:\n",
        "    # Compute accuracy on the test set\n",
        "\n",
        "    accuracy =\n",
        "    test_accuracy.append(accuracy)\n",
        "    print(f'Accuracy on the test set: {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "qKGfuOMeQ_JN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.1.3 Plot training and test accuracy. [2 pts]\n",
        "\n",
        "\n",
        "In your training loop above, make sure to calculate training accuracy per epoch and append it to the `train_accuracy` and `test_accuracy` variables.\n",
        "\n",
        "We have written the plotting code in the cell below.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b03V5xS2-jSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "axs[0].plot(train_accuracy, label='Train Accuracy', color='blue')\n",
        "axs[0].set_title('Train Accuracy vs. Epoch')\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[0].set_ylabel('Accuracy (%)')\n",
        "axs[0].grid(True)\n",
        "axs[0].legend()\n",
        "\n",
        "axs[1].plot(test_accuracy, label='Test Accuracy', color='green')\n",
        "axs[1].set_title('Test Accuracy vs. Epoch')\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[1].set_ylabel('Accuracy (%)')\n",
        "axs[1].grid(True)\n",
        "axs[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dmJrdp2JP3up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Generalization vs Memorization [7 pts]\n",
        "\n",
        "In this experiment, we explore the capacity of deep neural networks to memorize arbitrary data by training a model on randomly shuffled labels. Both the training and test labels are randomized. The goal is to observe the model’s behavior when trained and tested on randomly shuffled labels.\n",
        "\n",
        "**Your model should be able to memorize the random training data. That means you should achieve high training accuracy.**\n",
        "\n",
        "\n",
        "For more details on label randomization experiments, refer to [2].\n",
        "\n",
        "\n",
        "**Reference**\n",
        "[2] Zhang, Chiyuan, et al. \"Understanding deep learning requires rethinking generalization.\" International Conference on Learning Representations. 2022."
      ],
      "metadata": {
        "id": "Tau2IRc2zrzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset and dataloader setup\n",
        "\n",
        "Here, we will create a dataset that has random labels."
      ],
      "metadata": {
        "id": "bGxrEBcBCZoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "\n",
        "random_train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "random_test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Apply random labels to the datasets\n",
        "num_classes = 10\n",
        "random_train_data.targets = np.random.randint(0, num_classes, size=len(random_train_data))\n",
        "random_test_data.targets = np.random.randint(0, num_classes, size=len(random_test_data))\n",
        "\n",
        "# To make the training simpler, let us subsample the dataset and pick only 5000 training and 500 test images\n",
        "train_subset_indices = torch.randperm(len(random_train_data))[:5000]\n",
        "test_subset_indices = torch.randperm(len(random_test_data))[:500]\n",
        "\n",
        "random_train_data = Subset(random_train_data, train_subset_indices)\n",
        "random_test_data = Subset(random_test_data, test_subset_indices)\n",
        "\n",
        "batch_size = 8\n",
        "random_train_loader = DataLoader(random_train_data, batch_size=batch_size, shuffle=True)\n",
        "random_test_loader = DataLoader(random_test_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "94whDRclCWfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.2.1 Implement training and evaluation [3 pts]\n",
        "\n",
        "Write a training loop similar to ***Question 5.1.2*** but instead use `random_train_loader` and `random_test_loader` data loaders.\n",
        "\n",
        "**Make sure to create a new model object.**\n",
        "\n",
        "\n",
        "Compute and store both test accuracy and training accuracy for this randomized experiment at every epoch.\n",
        "\n",
        "**Hint**. Your model should be large enough to memorize training data."
      ],
      "metadata": {
        "id": "voiOK17LCPlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# Write training loop"
      ],
      "metadata": {
        "id": "G7wsRHTVCUo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.2.3 Plot training and test accuracy for the randomized training. [2pts]\n"
      ],
      "metadata": {
        "id": "6w87xQR7GQl8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bh0awrgBGLL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.2.4 Generalization Capability of the Model [2 pts]\n",
        "\n",
        "Discuss your observations during the randomization test. What did you notice about the model's ability to fit the training data and generalize to the test set when trained on randomized labels?\n",
        "\n",
        "**Answer:**"
      ],
      "metadata": {
        "id": "HEGd6iypOpTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **Submission instructions**\n",
        "1. Download this Colab to ipynb, and convert it to PDF. Follow similar steps as [here](https://stackoverflow.com/questions/53460051/convert-ipynb-notebook-to-html-in-google-colab) but convert to PDF.\n",
        " - Download your .ipynb file. You can do it using only Google Colab. `File` -> `Download` -> `Download .ipynb`\n",
        " - Reupload it so Colab can see it. Click on the `Files` icon on the far left to expand the side bar. You can directly drag the downloaded .ipynb file to the area. Or click `Upload to session storage` icon and then select & upload your .ipynb file.\n",
        " - Conversion using %%shell.\n",
        " ```\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-plain-generic pandoc\n",
        "!pip install pypandoc\n",
        "!jupyter nbconvert --log-level CRITICAL --to pdf name_of_hw.ipynb\n",
        "  ```\n",
        " - Your PDF file is ready. Click 3 dots and `Download`.\n",
        "**Note: Please follow these instructions to generate the PDF. Do not use any other method, such as `ctrl+p`.**\n",
        "\n",
        "2. Upload the PDF to elearn and **select** the correct pages for each question. Refer to the week 1 discussion video or contact the TAs if you face any issues. **Important!**\n",
        "\n",
        "\n",
        "3. Upload the `.ipynb` file to elearn. Make sure that both the **code** and the **PDF** are uploaded. **Important!**\n",
        "\n",
        "\n",
        "Notice: In case of errors in conversion, please check your LaTeX and debug. In Markdown, when you write in LaTeX math mode, do not leave any leading and trailing whitespaces inside the dollar signs ($). For example, write `(dollarSign)\\mathbf(dollarSign)(dollarSign)` instead of `(dollarSign)(space)\\mathbf{w}(dollarSign)`. Otherwise, nbconvert will throw an error and the generated pdf will be incomplete."
      ],
      "metadata": {
        "id": "EvKaIDEkX0jZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-plain-generic pandoc\n",
        "!pip install pypandoc"
      ],
      "metadata": {
        "id": "jdyFU83rX0jZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb5e90e5-5a5f-47a0-9df9-d098fc1386b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [75.2 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,604 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,842 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,788 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,542 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,243 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,259 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,148 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,696 kB]\n",
            "Fetched 26.5 MB in 7s (3,678 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  dvisvgm fonts-droid-fallback fonts-lato fonts-lmodern fonts-noto-mono\n",
            "  fonts-texgyre fonts-urw-base35 libapache-pom-java\n",
            "  libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3\n",
            "  libcommons-logging-java libcommons-parent-java libfontbox-java libgs9\n",
            "  libgs9-common libidn12 libijs-0.35 libjbig2dec0 libkpathsea6 libpdfbox-java\n",
            "  libptexenc1 libruby3.0 libsynctex2 libteckit0 libtexlua53 libtexluajit2\n",
            "  libwoff1 libzzip-0-13 lmodern pandoc-data poppler-data preview-latex-style\n",
            "  rake ruby ruby-net-telnet ruby-rubygems ruby-webrick ruby-xmlrpc ruby3.0\n",
            "  rubygems-integration t1utils teckit tex-common tex-gyre texlive-base\n",
            "  texlive-binaries texlive-latex-base texlive-latex-extra\n",
            "  texlive-latex-recommended texlive-pictures tipa xfonts-encodings\n",
            "  xfonts-utils\n",
            "Suggested packages:\n",
            "  fonts-noto fonts-freefont-otf | fonts-freefont-ttf libavalon-framework-java\n",
            "  libcommons-logging-java-doc libexcalibur-logkit-java liblog4j1.2-java\n",
            "  texlive-luatex pandoc-citeproc context wkhtmltopdf librsvg2-bin groff ghc\n",
            "  nodejs php python libjs-mathjax libjs-katex citation-style-language-styles\n",
            "  poppler-utils ghostscript fonts-japanese-mincho | fonts-ipafont-mincho\n",
            "  fonts-japanese-gothic | fonts-ipafont-gothic fonts-arphic-ukai\n",
            "  fonts-arphic-uming fonts-nanum ri ruby-dev bundler debhelper gv\n",
            "  | postscript-viewer perl-tk xpdf | pdf-viewer xzdec\n",
            "  texlive-fonts-recommended-doc texlive-latex-base-doc python3-pygments\n",
            "  icc-profiles libfile-which-perl libspreadsheet-parseexcel-perl\n",
            "  texlive-latex-extra-doc texlive-latex-recommended-doc texlive-pstricks\n",
            "  dot2tex prerex texlive-pictures-doc vprerex default-jre-headless tipa-doc\n",
            "The following NEW packages will be installed:\n",
            "  dvisvgm fonts-droid-fallback fonts-lato fonts-lmodern fonts-noto-mono\n",
            "  fonts-texgyre fonts-urw-base35 libapache-pom-java\n",
            "  libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3\n",
            "  libcommons-logging-java libcommons-parent-java libfontbox-java libgs9\n",
            "  libgs9-common libidn12 libijs-0.35 libjbig2dec0 libkpathsea6 libpdfbox-java\n",
            "  libptexenc1 libruby3.0 libsynctex2 libteckit0 libtexlua53 libtexluajit2\n",
            "  libwoff1 libzzip-0-13 lmodern pandoc pandoc-data poppler-data\n",
            "  preview-latex-style rake ruby ruby-net-telnet ruby-rubygems ruby-webrick\n",
            "  ruby-xmlrpc ruby3.0 rubygems-integration t1utils teckit tex-common tex-gyre\n",
            "  texlive-base texlive-binaries texlive-fonts-recommended texlive-latex-base\n",
            "  texlive-latex-extra texlive-latex-recommended texlive-pictures\n",
            "  texlive-plain-generic texlive-xetex tipa xfonts-encodings xfonts-utils\n",
            "0 upgraded, 57 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 202 MB of archives.\n",
            "After this operation, 728 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1build1 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-lato all 2.0-2.1 [2,696 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 poppler-data all 0.4.11-1 [2,171 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tex-common all 6.17 [33.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-urw-base35 all 20200910-1 [6,367 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9-common all 9.55.0~dfsg1-0ubuntu5.11 [753 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libidn12 amd64 1.38-4ubuntu1 [60.0 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libijs-0.35 amd64 0.35-15build2 [16.5 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjbig2dec0 amd64 0.19-3build2 [64.7 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9 amd64 9.55.0~dfsg1-0ubuntu5.11 [5,031 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libkpathsea6 amd64 2021.20210626.59705-1ubuntu0.2 [60.4 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwoff1 amd64 1.0.2-1build4 [45.2 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 dvisvgm amd64 2.13.1-1 [1,221 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-lmodern all 2.004.5-6.1 [4,532 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-mono all 20201225-1build1 [397 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-texgyre all 20180621-3.1 [10.2 MB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libapache-pom-java all 18-1 [4,720 B]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [115 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm-extensions0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [25.1 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcommons-parent-java all 43-1 [10.8 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcommons-logging-java all 1.2-2 [60.3 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libptexenc1 amd64 2021.20210626.59705-1ubuntu0.2 [39.1 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy/main amd64 rubygems-integration all 1.18 [5,336 B]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby3.0 amd64 3.0.2-7ubuntu2.10 [50.1 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby-rubygems all 3.3.5-2 [228 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby amd64 1:3.0~exp1 [5,100 B]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 rake all 13.0.6-2 [61.7 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby-net-telnet all 0.1.1-2 [12.6 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby-webrick all 1.7.0-3ubuntu0.1 [52.1 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby-xmlrpc all 0.3.2-1ubuntu0.1 [24.9 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libruby3.0 amd64 3.0.2-7ubuntu2.10 [5,114 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libsynctex2 amd64 2021.20210626.59705-1ubuntu0.2 [55.6 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libteckit0 amd64 2.5.11+ds1-1 [421 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtexlua53 amd64 2021.20210626.59705-1ubuntu0.2 [120 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtexluajit2 amd64 2021.20210626.59705-1ubuntu0.2 [267 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libzzip-0-13 amd64 0.13.72+dfsg.1-1.1 [27.0 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu jammy/universe amd64 lmodern all 2.004.5-6.1 [9,471 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc-data all 2.9.2.1-3ubuntu2 [81.8 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc amd64 2.9.2.1-3ubuntu2 [20.3 MB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu jammy/universe amd64 preview-latex-style all 12.2-1ubuntu1 [185 kB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu jammy/main amd64 t1utils amd64 1.41-4build2 [61.3 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu jammy/universe amd64 teckit amd64 2.5.11+ds1-1 [699 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tex-gyre all 20180621-3.1 [6,209 kB]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 texlive-binaries amd64 2021.20210626.59705-1ubuntu0.2 [9,860 kB]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-base all 2021.20220204-1 [21.0 MB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-fonts-recommended all 2021.20220204-1 [4,972 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-base all 2021.20220204-1 [1,128 kB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfontbox-java all 1:1.8.16-2 [207 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpdfbox-java all 1:1.8.16-2 [5,199 kB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-recommended all 2021.20220204-1 [14.4 MB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-pictures all 2021.20220204-1 [8,720 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-extra all 2021.20220204-1 [13.9 MB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-plain-generic all 2021.20220204-1 [27.5 MB]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tipa all 2:1.3-21 [2,967 kB]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-xetex all 2021.20220204-1 [12.4 MB]\n",
            "Fetched 202 MB in 15s (13.4 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 57.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 126332 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1build1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Selecting previously unselected package fonts-lato.\n",
            "Preparing to unpack .../01-fonts-lato_2.0-2.1_all.deb ...\n",
            "Unpacking fonts-lato (2.0-2.1) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../02-poppler-data_0.4.11-1_all.deb ...\n",
            "Unpacking poppler-data (0.4.11-1) ...\n",
            "Selecting previously unselected package tex-common.\n",
            "Preparing to unpack .../03-tex-common_6.17_all.deb ...\n",
            "Unpacking tex-common (6.17) ...\n",
            "Selecting previously unselected package fonts-urw-base35.\n",
            "Preparing to unpack .../04-fonts-urw-base35_20200910-1_all.deb ...\n",
            "Unpacking fonts-urw-base35 (20200910-1) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../05-libgs9-common_9.55.0~dfsg1-0ubuntu5.11_all.deb ...\n",
            "Unpacking libgs9-common (9.55.0~dfsg1-0ubuntu5.11) ...\n",
            "Selecting previously unselected package libidn12:amd64.\n",
            "Preparing to unpack .../06-libidn12_1.38-4ubuntu1_amd64.deb ...\n",
            "Unpacking libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../07-libijs-0.35_0.35-15build2_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../08-libjbig2dec0_0.19-3build2_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../09-libgs9_9.55.0~dfsg1-0ubuntu5.11_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.11) ...\n",
            "Selecting previously unselected package libkpathsea6:amd64.\n",
            "Preparing to unpack .../10-libkpathsea6_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libwoff1:amd64.\n",
            "Preparing to unpack .../11-libwoff1_1.0.2-1build4_amd64.deb ...\n",
            "Unpacking libwoff1:amd64 (1.0.2-1build4) ...\n",
            "Selecting previously unselected package dvisvgm.\n",
            "Preparing to unpack .../12-dvisvgm_2.13.1-1_amd64.deb ...\n",
            "Unpacking dvisvgm (2.13.1-1) ...\n",
            "Selecting previously unselected package fonts-lmodern.\n",
            "Preparing to unpack .../13-fonts-lmodern_2.004.5-6.1_all.deb ...\n",
            "Unpacking fonts-lmodern (2.004.5-6.1) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../14-fonts-noto-mono_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-mono (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-texgyre.\n",
            "Preparing to unpack .../15-fonts-texgyre_20180621-3.1_all.deb ...\n",
            "Unpacking fonts-texgyre (20180621-3.1) ...\n",
            "Selecting previously unselected package libapache-pom-java.\n",
            "Preparing to unpack .../16-libapache-pom-java_18-1_all.deb ...\n",
            "Unpacking libapache-pom-java (18-1) ...\n",
            "Selecting previously unselected package libcmark-gfm0.29.0.gfm.3:amd64.\n",
            "Preparing to unpack .../17-libcmark-gfm0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package libcmark-gfm-extensions0.29.0.gfm.3:amd64.\n",
            "Preparing to unpack .../18-libcmark-gfm-extensions0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package libcommons-parent-java.\n",
            "Preparing to unpack .../19-libcommons-parent-java_43-1_all.deb ...\n",
            "Unpacking libcommons-parent-java (43-1) ...\n",
            "Selecting previously unselected package libcommons-logging-java.\n",
            "Preparing to unpack .../20-libcommons-logging-java_1.2-2_all.deb ...\n",
            "Unpacking libcommons-logging-java (1.2-2) ...\n",
            "Selecting previously unselected package libptexenc1:amd64.\n",
            "Preparing to unpack .../21-libptexenc1_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libptexenc1:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package rubygems-integration.\n",
            "Preparing to unpack .../22-rubygems-integration_1.18_all.deb ...\n",
            "Unpacking rubygems-integration (1.18) ...\n",
            "Selecting previously unselected package ruby3.0.\n",
            "Preparing to unpack .../23-ruby3.0_3.0.2-7ubuntu2.10_amd64.deb ...\n",
            "Unpacking ruby3.0 (3.0.2-7ubuntu2.10) ...\n",
            "Selecting previously unselected package ruby-rubygems.\n",
            "Preparing to unpack .../24-ruby-rubygems_3.3.5-2_all.deb ...\n",
            "Unpacking ruby-rubygems (3.3.5-2) ...\n",
            "Selecting previously unselected package ruby.\n",
            "Preparing to unpack .../25-ruby_1%3a3.0~exp1_amd64.deb ...\n",
            "Unpacking ruby (1:3.0~exp1) ...\n",
            "Selecting previously unselected package rake.\n",
            "Preparing to unpack .../26-rake_13.0.6-2_all.deb ...\n",
            "Unpacking rake (13.0.6-2) ...\n",
            "Selecting previously unselected package ruby-net-telnet.\n",
            "Preparing to unpack .../27-ruby-net-telnet_0.1.1-2_all.deb ...\n",
            "Unpacking ruby-net-telnet (0.1.1-2) ...\n",
            "Selecting previously unselected package ruby-webrick.\n",
            "Preparing to unpack .../28-ruby-webrick_1.7.0-3ubuntu0.1_all.deb ...\n",
            "Unpacking ruby-webrick (1.7.0-3ubuntu0.1) ...\n",
            "Selecting previously unselected package ruby-xmlrpc.\n",
            "Preparing to unpack .../29-ruby-xmlrpc_0.3.2-1ubuntu0.1_all.deb ...\n",
            "Unpacking ruby-xmlrpc (0.3.2-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libruby3.0:amd64.\n",
            "Preparing to unpack .../30-libruby3.0_3.0.2-7ubuntu2.10_amd64.deb ...\n",
            "Unpacking libruby3.0:amd64 (3.0.2-7ubuntu2.10) ...\n",
            "Selecting previously unselected package libsynctex2:amd64.\n",
            "Preparing to unpack .../31-libsynctex2_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libteckit0:amd64.\n",
            "Preparing to unpack .../32-libteckit0_2.5.11+ds1-1_amd64.deb ...\n",
            "Unpacking libteckit0:amd64 (2.5.11+ds1-1) ...\n",
            "Selecting previously unselected package libtexlua53:amd64.\n",
            "Preparing to unpack .../33-libtexlua53_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libtexlua53:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libtexluajit2:amd64.\n",
            "Preparing to unpack .../34-libtexluajit2_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libtexluajit2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libzzip-0-13:amd64.\n",
            "Preparing to unpack .../35-libzzip-0-13_0.13.72+dfsg.1-1.1_amd64.deb ...\n",
            "Unpacking libzzip-0-13:amd64 (0.13.72+dfsg.1-1.1) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../36-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../37-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package lmodern.\n",
            "Preparing to unpack .../38-lmodern_2.004.5-6.1_all.deb ...\n",
            "Unpacking lmodern (2.004.5-6.1) ...\n",
            "Selecting previously unselected package pandoc-data.\n",
            "Preparing to unpack .../39-pandoc-data_2.9.2.1-3ubuntu2_all.deb ...\n",
            "Unpacking pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Selecting previously unselected package pandoc.\n",
            "Preparing to unpack .../40-pandoc_2.9.2.1-3ubuntu2_amd64.deb ...\n",
            "Unpacking pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Selecting previously unselected package preview-latex-style.\n",
            "Preparing to unpack .../41-preview-latex-style_12.2-1ubuntu1_all.deb ...\n",
            "Unpacking preview-latex-style (12.2-1ubuntu1) ...\n",
            "Selecting previously unselected package t1utils.\n",
            "Preparing to unpack .../42-t1utils_1.41-4build2_amd64.deb ...\n",
            "Unpacking t1utils (1.41-4build2) ...\n",
            "Selecting previously unselected package teckit.\n",
            "Preparing to unpack .../43-teckit_2.5.11+ds1-1_amd64.deb ...\n",
            "Unpacking teckit (2.5.11+ds1-1) ...\n",
            "Selecting previously unselected package tex-gyre.\n",
            "Preparing to unpack .../44-tex-gyre_20180621-3.1_all.deb ...\n",
            "Unpacking tex-gyre (20180621-3.1) ...\n",
            "Selecting previously unselected package texlive-binaries.\n",
            "Preparing to unpack .../45-texlive-binaries_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking texlive-binaries (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package texlive-base.\n",
            "Preparing to unpack .../46-texlive-base_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-base (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-fonts-recommended.\n",
            "Preparing to unpack .../47-texlive-fonts-recommended_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-fonts-recommended (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-latex-base.\n",
            "Preparing to unpack .../48-texlive-latex-base_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-latex-base (2021.20220204-1) ...\n",
            "Selecting previously unselected package libfontbox-java.\n",
            "Preparing to unpack .../49-libfontbox-java_1%3a1.8.16-2_all.deb ...\n",
            "Unpacking libfontbox-java (1:1.8.16-2) ...\n",
            "Selecting previously unselected package libpdfbox-java.\n",
            "Preparing to unpack .../50-libpdfbox-java_1%3a1.8.16-2_all.deb ...\n",
            "Unpacking libpdfbox-java (1:1.8.16-2) ...\n",
            "Selecting previously unselected package texlive-latex-recommended.\n",
            "Preparing to unpack .../51-texlive-latex-recommended_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-latex-recommended (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-pictures.\n",
            "Preparing to unpack .../52-texlive-pictures_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-pictures (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-latex-extra.\n",
            "Preparing to unpack .../53-texlive-latex-extra_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-latex-extra (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-plain-generic.\n",
            "Preparing to unpack .../54-texlive-plain-generic_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-plain-generic (2021.20220204-1) ...\n",
            "Selecting previously unselected package tipa.\n",
            "Preparing to unpack .../55-tipa_2%3a1.3-21_all.deb ...\n",
            "Unpacking tipa (2:1.3-21) ...\n",
            "Selecting previously unselected package texlive-xetex.\n",
            "Preparing to unpack .../56-texlive-xetex_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-xetex (2021.20220204-1) ...\n",
            "Setting up fonts-lato (2.0-2.1) ...\n",
            "Setting up fonts-noto-mono (20201225-1build1) ...\n",
            "Setting up libwoff1:amd64 (1.0.2-1build4) ...\n",
            "Setting up libtexlua53:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Setting up libtexluajit2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up libfontbox-java (1:1.8.16-2) ...\n",
            "Setting up rubygems-integration (1.18) ...\n",
            "Setting up libzzip-0-13:amd64 (0.13.72+dfsg.1-1.1) ...\n",
            "Setting up fonts-urw-base35 (20200910-1) ...\n",
            "Setting up poppler-data (0.4.11-1) ...\n",
            "Setting up tex-common (6.17) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "update-language: texlive-base not installed and configured, doing nothing!\n",
            "Setting up libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Setting up libteckit0:amd64 (2.5.11+ds1-1) ...\n",
            "Setting up libapache-pom-java (18-1) ...\n",
            "Setting up ruby-net-telnet (0.1.1-2) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up t1utils (1.41-4build2) ...\n",
            "Setting up libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Setting up fonts-texgyre (20180621-3.1) ...\n",
            "Setting up libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up ruby-webrick (1.7.0-3ubuntu0.1) ...\n",
            "Setting up libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up fonts-lmodern (2.004.5-6.1) ...\n",
            "Setting up libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Setting up pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Setting up ruby-xmlrpc (0.3.2-1ubuntu0.1) ...\n",
            "Setting up libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up libgs9-common (9.55.0~dfsg1-0ubuntu5.11) ...\n",
            "Setting up teckit (2.5.11+ds1-1) ...\n",
            "Setting up libpdfbox-java (1:1.8.16-2) ...\n",
            "Setting up libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.11) ...\n",
            "Setting up preview-latex-style (12.2-1ubuntu1) ...\n",
            "Setting up libcommons-parent-java (43-1) ...\n",
            "Setting up dvisvgm (2.13.1-1) ...\n",
            "Setting up libcommons-logging-java (1.2-2) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up libptexenc1:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Setting up texlive-binaries (2021.20210626.59705-1ubuntu0.2) ...\n",
            "update-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode\n",
            "update-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode\n",
            "Setting up lmodern (2.004.5-6.1) ...\n",
            "Setting up texlive-base (2021.20220204-1) ...\n",
            "/usr/bin/ucfr\n",
            "/usr/bin/ucfr\n",
            "/usr/bin/ucfr\n",
            "/usr/bin/ucfr\n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R... \n",
            "mktexlsr: Done.\n",
            "tl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps\n",
            "tl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg\n",
            "tl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper\n",
            "tl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/tex-ini-files/pdftexconfig.tex\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up tex-gyre (20180621-3.1) ...\n",
            "Setting up texlive-plain-generic (2021.20220204-1) ...\n",
            "Setting up texlive-latex-base (2021.20220204-1) ...\n",
            "Setting up texlive-latex-recommended (2021.20220204-1) ...\n",
            "Setting up texlive-pictures (2021.20220204-1) ...\n",
            "Setting up texlive-fonts-recommended (2021.20220204-1) ...\n",
            "Setting up tipa (2:1.3-21) ...\n",
            "Setting up texlive-latex-extra (2021.20220204-1) ...\n",
            "Setting up texlive-xetex (2021.20220204-1) ...\n",
            "Setting up rake (13.0.6-2) ...\n",
            "Setting up libruby3.0:amd64 (3.0.2-7ubuntu2.10) ...\n",
            "Setting up ruby3.0 (3.0.2-7ubuntu2.10) ...\n",
            "Setting up ruby (1:3.0~exp1) ...\n",
            "Setting up ruby-rubygems (3.3.5-2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "Processing triggers for tex-common (6.17) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Running updmap-sys. This may take some time... done.\n",
            "Running mktexlsr /var/lib/texmf ... done.\n",
            "Building format(s) --all.\n",
            "\tThis may take some time... done.\n",
            "Collecting pypandoc\n",
            "  Downloading pypandoc-1.15-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading pypandoc-1.15-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: pypandoc\n",
            "Successfully installed pypandoc-1.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --log-level INFO --to pdf Spring2025_hw1.ipynb # make sure the ipynb name is correct"
      ],
      "metadata": {
        "id": "M2XuF-qZ_-fk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5496e806-c291-409c-c779-4d375c986297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook Spring2025_hw1.ipynb to pdf\n",
            "[NbConvertApp] Writing 83837 bytes to notebook.tex\n",
            "[NbConvertApp] Building PDF\n",
            "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n",
            "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
            "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
            "[NbConvertApp] PDF successfully created\n",
            "[NbConvertApp] Writing 120043 bytes to Spring2025_hw1.pdf\n"
          ]
        }
      ]
    }
  ]
}